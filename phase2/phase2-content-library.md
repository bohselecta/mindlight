# Reflector Content Library
## Micro-Explainers (MDX Format)

Each explainer: 120-200 words, 45-90 seconds reading time, mechanism-focused (not tribal).

---

## /content/explainers/echo-chambers.mdx

```mdx
---
title: "Echo Chambers"
category: "Information Patterns"
readTime: "60 seconds"
relatedModules: ["influence-map", "echo-loop"]
---

import { AlertCircle } from 'lucide-react'

# Echo Chambers

<div className="flex items-start gap-3 mb-4 p-4 bg-cyan-900/20 border border-cyan-700/30 rounded-xl">
  <AlertCircle className="w-5 h-5 text-cyan-400 flex-shrink-0 mt-0.5" />
  <p className="text-sm text-cyan-300">
    An echo chamber isn't just "hearing what you agree with"—it's a 
    **self-reinforcing loop** where information inputs confirm existing beliefs 
    without exposure to meaningful counter-evidence.
  </p>
</div>

### How It Forms

Algorithms prioritize engagement → Emotional content spreads faster → 
You click what resonates → More of the same appears → Your perception 
narrows without realizing it.

### The Mechanism

Echo chambers don't create beliefs—they **calcify** them. Over time:
- Uncertainty decreases (you feel more certain)
- Nuance disappears (positions become binary)
- Out-groups dehumanize (they become "irrational")

### Breaking Out

The antidote isn't "hearing both sides"—it's **procedural diversity**:
- Seek sources with different *methods* (academic vs journalistic vs experiential)
- Notice when you feel defensive (that's the echo wall)
- Ask: "What would I need to see to change my mind?"

<div className="mt-6 p-4 bg-slate-800/50 rounded-xl">
  <p className="text-sm text-slate-400">
    <strong className="text-slate-300">Try this:</strong> Use the Influence Map 
    to visualize your information network. High homophily? That's your cue.
  </p>
</div>
```

---

## /content/explainers/confirmation-bias.mdx

```mdx
---
title: "Confirmation Bias"
category: "Cognitive Patterns"
readTime: "75 seconds"
relatedModules: ["echo-loop", "disconfirm"]
---

# Confirmation Bias

**You notice evidence that confirms your beliefs more than evidence that challenges them.** 
This isn't a character flaw—it's a cognitive default.

### Why It Happens

Your brain is a pattern-recognition machine optimized for efficiency, not accuracy. 
Challenging a belief requires mental energy. Confirming it feels like progress.

Result: You unconsciously weight supportive evidence higher and dismiss contradictory 
data as "outliers" or "biased sources."

### The Stealth Factor

Confirmation bias is invisible *to the person experiencing it*. Everyone thinks 
they're being rational. The real test: **can you list what would change your mind?**

If your answer is vague ("nothing" or "I'd need overwhelming proof"), you're in 
confirmation lock.

### Self-Correction

1. **Falsification practice**: Before defending a belief, list 3 conditions that would disprove it
2. **Seek disconfirmation**: Actively search for the *best* opposing argument (not the weakest strawman)
3. **Track your certainty**: High certainty + low exposure to counterevidence = red flag

<div className="mt-6 p-4 bg-violet-900/20 border border-violet-700/30 rounded-xl">
  <p className="text-sm text-violet-300">
    <strong className="text-violet-200">Research finding:</strong> People who 
    practice listing falsifiers score 23% higher on critical thinking assessments 
    (Stanovich, 2016).
  </p>
</div>
```

---

## /content/explainers/identity-fusion.mdx

```mdx
---
title: "Identity Fusion"
category: "Social Psychology"
readTime: "90 seconds"
relatedModules: ["identity-mirror", "schema-reclaim"]
---

import { Users } from 'lucide-react'

# Identity Fusion

<div className="flex items-center gap-3 mb-4">
  <Users className="w-6 h-6 text-rose-400" />
  <h3 className="text-lg text-slate-200">When beliefs become who you are</h3>
</div>

Identity fusion happens when your **personal identity** merges with a **group identity** 
so thoroughly that challenging the group feels like a personal attack.

### The Process

1. **Boundary dissolution**: "I believe X" becomes "I *am* an X-ist"
2. **Loyalty intensifies**: Defending the group feels like self-defense
3. **Criticism = threat**: Disagreement registers as danger, not debate

### Why It Matters

Fused identities create **belief rigidity**. You can't revise a position without 
feeling like you're betraying yourself or your tribe.

Example: "I'm a progressive" (identity) vs "I support progressive policies" (position). 
The latter can change with evidence. The former cannot without identity crisis.

### The Schema Connection

Identity fusion often stems from:
- **Approval-seeking**: Need for belonging drives fusion
- **Defectiveness**: "If I'm wrong, I'm worthless"
- **Dependence**: Outsourcing judgment to the group

### Reclaiming Boundaries

Ask yourself:
- Do I hold this belief because of *evidence* or *belonging*?
- Would I still believe this if I left my current community?
- Can I separate my values from my tribal labels?

<div className="mt-6 p-4 bg-rose-900/20 border border-rose-700/30 rounded-xl">
  <p className="text-sm text-rose-300">
    <strong className="text-rose-200">Key insight:</strong> You can value a 
    community without outsourcing your epistemic autonomy to it.
  </p>
</div>
```

---

## /content/explainers/motivated-reasoning.mdx

```mdx
---
title: "Motivated Reasoning"
category: "Cognitive Patterns"
readTime: "75 seconds"
relatedModules: ["schema-reclaim", "disconfirm"]
---

# Motivated Reasoning

**Conclusion first, justification second.** Motivated reasoning is when you 
unconsciously work backwards from a desired conclusion, gathering "evidence" 
that supports what you already want to believe.

### How It Works

1. Encounter new information
2. Emotional reaction (do I *want* this to be true?)
3. Cognitive search (find reasons to support preferred conclusion)
4. Rationalization ("I'm being objective")

You genuinely believe you're reasoning logically—but the emotional preference 
set the destination before the journey began.

### The Emotional Tells

Watch for these patterns:
- **Relief** when "your side" is vindicated
- **Anxiety** when contradictory evidence appears
- **Creativity spike** when defending a preferred belief (you suddenly find 10 reasons)
- **Rigid certainty** despite thin evidence

If you feel *invested* in a conclusion being true, motivated reasoning is likely active.

### Decoupling Practice

1. Notice the emotional charge *before* evaluating evidence
2. Ask: "Would I accept this reasoning if it led to the opposite conclusion?"
3. Steelman the position you *don't* want to be true

<div className="mt-6 p-4 bg-amber-900/20 border border-amber-700/30 rounded-xl">
  <p className="text-sm text-amber-300">
    <strong className="text-amber-200">Honest question:</strong> What belief 
    would you be most *upset* to discover was false? That's where motivated 
    reasoning lives.
  </p>
</div>
```

---

## /content/explainers/certainty-addiction.mdx

```mdx
---
title: "Certainty Addiction"
category: "Cognitive Patterns"
readTime: "60 seconds"
relatedModules: ["disconfirm", "schema-reclaim"]
---

# Certainty Addiction

Some people crave certainty the way others crave sugar. Ambiguity feels *wrong*. 
Nuance feels weak. "I don't know" feels like failure.

### The Psychology

High need for cognitive closure = discomfort with uncertainty. Your brain wants 
**answers now**, even if incomplete evidence suggests waiting would be wiser.

This isn't about intelligence—it's about tolerance for ambiguity.

### The Trap

Certainty addiction makes you:
- Accept weak evidence if it provides closure
- Reject strong evidence if it creates doubt
- Confuse confidence with correctness

Result: You feel certain, but you're often *precisely wrong* instead of *approximately right*.

### The Antidote

Practice saying:
- "I don't know yet"
- "The evidence is mixed"
- "I'm 60% confident, not 100%"

Uncertainty isn't weakness—it's **epistemic honesty**.

<div className="mt-6 p-4 bg-blue-900/20 border border-blue-700/30 rounded-xl">
  <p className="text-sm text-blue-300">
    <strong className="text-blue-200">Research:</strong> People high in need 
    for closure are 3x more susceptible to misinformation (Kruglanski et al., 2020).
  </p>
</div>
```

---

## /content/explainers/epistemic-humility.mdx

```mdx
---
title: "Epistemic Humility"
category: "Cognitive Virtues"
readTime: "60 seconds"
relatedModules: ["disconfirm", "baseline-mirror"]
---

# Epistemic Humility

**Knowing what you don't know.** Epistemic humility is the capacity to hold 
beliefs provisionally, recognize your cognitive limits, and update when evidence shifts.

### What It's NOT

- ❌ Wishy-washy relativism ("all opinions are equal")
- ❌ Performative uncertainty ("I have no views")
- ❌ Self-deprecation ("I'm probably wrong about everything")

### What It IS

- ✅ Calibrated confidence (strong beliefs on strong evidence, weak beliefs on weak evidence)
- ✅ Falsifiability (knowing what would change your mind)
- ✅ Intellectual courage (admitting error without identity collapse)

### The Paradox

High epistemic humility often correlates with *deeper expertise*. Experts know 
the complexity and uncertainty. Novices think it's simple.

Dunning-Kruger in action: Low knowledge → high certainty. High knowledge → moderate certainty.

### Practice

Ask yourself:
- On a scale of 1-10, how certain am I?
- What's my confidence interval? (60% sure? 90%?)
- If I'm wrong, what would that look like?

<div className="mt-6 p-4 bg-emerald-900/20 border border-emerald-700/30 rounded-xl">
  <p className="text-sm text-emerald-300">
    <strong className="text-emerald-200">Goal:</strong> Be precisely uncertain 
    rather than confidently wrong.
  </p>
</div>
```

---

## /content/explainers/source-heuristics.mdx

```mdx
---
title: "Source Heuristics"
category: "Information Patterns"
readTime: "75 seconds"
relatedModules: ["influence-map", "echo-loop"]
---

# Source Heuristics

You trust information based on *who said it* more than *what the evidence shows*. 
This is efficient but exploitable.

### The Mechanism

Your brain uses shortcuts:
- **Authority**: "An expert said it" → probably true
- **Similarity**: "Someone like me said it" → relatable, trustworthy
- **Familiarity**: "I've heard this before" → must be accurate
- **Emotional resonance**: "This aligns with my values" → feels right

These heuristics work *most of the time*, but they're not truth detectors.

### The Hijack

- Charismatic figures exploit authority heuristics
- Echo chambers exploit similarity bias
- Repetition exploits familiarity (the "illusory truth effect")
- Tribal content exploits emotional resonance

Result: You feel like you're evaluating evidence, but you're actually outsourcing 
judgment to source credibility.

### Self-Check

Before accepting a claim, ask:
1. Who benefits if I believe this?
2. Am I trusting the person or the evidence?
3. Would I believe this from a source I dislike?

<div className="mt-6 p-4 bg-slate-800/50 rounded-xl">
  <p className="text-sm text-slate-400">
    <strong className="text-slate-300">Try this:</strong> Use Source Audit to 
    track where your beliefs originated. Pattern recognition often reveals 
    over-reliance on single sources.
  </p>
</div>
```

---

## /content/explainers/cognitive-inoculation.mdx

```mdx
---
title: "Cognitive Inoculation"
category: "Mental Resilience"
readTime: "75 seconds"
relatedModules: ["disconfirm", "echo-loop"]
---

# Cognitive Inoculation

Just like vaccines expose you to weakened pathogens to build immunity, **cognitive 
inoculation** exposes you to weak misinformation to strengthen your defenses.

### How It Works

1. Pre-exposure to manipulation tactics
2. Recognition training (spot the pattern)
3. Refutation practice (counter weak arguments)
4. Resilience building (harder to fool later)

Example: If you've practiced identifying emotional manipulation in headlines, 
you're less likely to fall for it in the wild.

### Why It's Powerful

Traditional fact-checking is reactive (debunking after belief formation). 
Inoculation is *preventive*—you develop pattern recognition before 
encountering persuasive content.

### What You're Inoculating Against

- Emotional manipulation ("be afraid/angry")
- False dichotomies ("you're either with us or against us")
- Strawman arguments (attacking weak versions of opposing views)
- Source laundering (obscuring where info came from)

### Practice

The Disconfirm Game and Echo-Loop train inoculation by:
- Exposing you to manipulative framing
- Making you conscious of your emotional reactions
- Rewarding pattern recognition

<div className="mt-6 p-4 bg-violet-900/20 border border-violet-700/30 rounded-xl">
  <p className="text-sm text-violet-300">
    <strong className="text-violet-200">Research:</strong> Inoculation reduces 
    susceptibility to misinformation by 21-33% (Roozenbeek & van der Linden, 2019).
  </p>
</div>
```

---

## /content/explainers/schema-therapy-intro.mdx

```mdx
---
title: "Emotional Schemas & Beliefs"
category: "Schema Therapy"
readTime: "90 seconds"
relatedModules: ["schema-reclaim"]
---

import { Heart, Shield, Zap, Brain } from 'lucide-react'

# Emotional Schemas & Beliefs

Your beliefs aren't just intellectual—they're often rooted in **emotional patterns** 
formed in childhood and reinforced over time.

### The Four Core Schemas (Reflector Focus)

<div className="grid grid-cols-2 gap-4 my-6">
  <div className="p-4 bg-rose-900/20 border border-rose-700/30 rounded-xl">
    <Heart className="w-5 h-5 text-rose-400 mb-2" />
    <div className="text-sm font-medium text-rose-300">Approval-Seeking</div>
    <p className="text-xs text-rose-400/70 mt-1">
      Need validation from authority → adopt their beliefs to feel worthy
    </p>
  </div>
  
  <div className="p-4 bg-blue-900/20 border border-blue-700/30 rounded-xl">
    <Shield className="w-5 h-5 text-blue-400 mb-2" />
    <div className="text-sm font-medium text-blue-300">Dependence</div>
    <p className="text-xs text-blue-400/70 mt-1">
      Trust group judgment over your own → outsource thinking
    </p>
  </div>
  
  <div className="p-4 bg-amber-900/20 border border-amber-700/30 rounded-xl">
    <Zap className="w-5 h-5 text-amber-400 mb-2" />
    <div className="text-sm font-medium text-amber-300">Punitiveness</div>
    <p className="text-xs text-amber-400/70 mt-1">
      Harsh reactions to dissent → defensive anger when challenged
    </p>
  </div>
  
  <div className="p-4 bg-violet-900/20 border border-violet-700/30 rounded-xl">
    <Brain className="w-5 h-5 text-violet-400 mb-2" />
    <div className="text-sm font-medium text-violet-300">Defectiveness</div>
    <p className="text-xs text-violet-400/70 mt-1">
      Being wrong = personal failure → cling to beliefs for self-worth
    </p>
  </div>
</div>

### Why This Matters

When a belief is challenged, schemas activate *before* rational analysis. You 
feel shame, anger, anxiety—and your thinking shuts down.

By recognizing the schema, you can **regulate the emotion first**, then 
re-examine the belief with a clearer mind.

### The Reclaim Process

1. Identify trigger ("Authority disagrees with me")
2. Notice emotion ("Shame, anxiety")
3. Regulate (60-second breathing)
4. Re-examine belief ("Do I still hold this with the same certainty?")

<div className="mt-6 p-4 bg-slate-800/50 rounded-xl">
  <p className="text-sm text-slate-400">
    <strong className="text-slate-300">Key principle:</strong> You can't think 
    clearly while emotionally hijacked. Regulate first, reason second.
  </p>
</div>
```

---

## Content Usage Guidelines

### Where to Display

1. **Home Feed**: Rotate one random explainer daily
2. **Post-Module**: Show relevant explainer after completion (e.g., "Identity Fusion" after Identity Mirror)
3. **Library**: Searchable collection in `/library` route
4. **Onboarding**: 2-3 key explainers during welcome flow

### Tone Enforcement

- ✅ Curious, respectful, evidence-based
- ✅ Mechanism-focused (not tribe-focused)
- ✅ Actionable ("Try this" endings)
- ❌ Never mention specific public figures
- ❌ Never use partisan language
- ❌ Never claim clinical expertise

### Expansion Plan

Target: 20+ explainers covering:
- Cognitive biases (anchoring, availability, sunk cost)
- Social dynamics (groupthink, polarization, tribalism)
- Information literacy (deepfakes, source verification, steelmanning)
- Emotional regulation (distress tolerance, mindfulness, reappraisal)

---

**These explainers are now ready for MDX rendering in your Next.js app.**
